<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <d-article> ## 1. Introduction The field of AI evaluation suffers from a "Cartesian Dualism." On one side, **Mechanistic Interpretability** studies the neurons and circuits of small models (micro-scale). On the other, **Behavioral Safety** benchmarks frontier models on static datasets (macro-scale). Rarely do these two worlds meet. This disconnect has obscured the root cause of **"Agent Collapse"**â€”the sudden, state-dependent loss of coherence where models fail to follow established constraints (e.g., JSON schemas) despite having the capability to do so. In this post, we propose a **Bidirectional** approach. We identify **Layer 1 Head 0 (L1H0)** in GPT-2 Small as a critical "Precursor Head" essential for In-Context Learning (ICL). We demonstrate that high-entropy adversarial inputs saturate this head, severing the model's ability to maintain context. Crucially, we find strong behavioral homology in frontier models (Claude 3.5 Sonnet), suggesting this mechanism is a universal vulnerability of Transformer attention dynamics. **TL;DR:** * **The Phenomenon:** High-entropy prefixes ("Rot") break schema adherence without altering instructions. * **The Mechanism:** Precursor heads (L1H0) fail to attend to the previous token when local entropy exceeds the signal strength. * **The Result:** A measurable phase transition from "Coherent Agent" to "Stochastic Generator." --- ## 2. The Physics of Meaning To understand collapse, we must look beyond "hallucination" as a psychological metaphor and treat it as a thermodynamic state. Recent work by **Apple Machine Learning Research** (Achiam et al., 2024) on *GSM-Symbolic* demonstrated that reasoning capabilities are fragile: minor perturbations in variables cause catastrophic accuracy drops <d-cite key="achiam2024gsm"></d-cite>. We posit that this fragility is the macroscopic symptom of **Precursor Head Saturation**. ### The Circuit: The Bucket Brigade Induction Headsâ€”the circuits responsible for copying patternsâ€”rely on a "Bucket Brigade" architecture <d-cite key="olsson2022context"></d-cite>: 1. **The Precursor (L1H0):** Moves information from the previous token ($t-1$) to the current residual stream ($t$). 2. **The Inductor (L5+):** Uses that information to query the distant past. If the Precursor drops the bucket, the Inductor has nothing to search for. ### The Collapse Threshold ($\Theta$) We model the Precursor's attention as a logistic competition between **Signal ($S$)** and **Entropy Load ($E$)**. $$P(\text{attend}) = \frac{1}{1 + e^{-(S - E)}}$$ * **Signal ($S$):** The dot product with the relevant token ($t-1$). * **Entropy Load ($E$):** The aggregate "loudness" of noise vectors (adversarial prefixes, glitch tokens). When $E &gt; S$, the probability of maintaining context drops to near zero. This explains why agents don't degrade gracefullyâ€”they snap. --- ## 3. Mechanism: L1H0 Saturation We designed the **Semiotic Adversarial Protocol (SAP)** to test this hypothesis. We inject a "Rot" prefixâ€”recursive logical paradoxes designed to maximize semantic entropyâ€”before a simple schema-copying task (`City -&gt; Country`). ### Micro-Scale: Circuit Trace (GPT-2 Small) Using `transformer_lens`, we traced the attention patterns of L1H0 across the schema region. The results from our **Unified Notebook** are stark: | Condition | Mean L1H0 Attention ($t-1$) | State | | :--- | :--- | :--- | | **Control** (Clean) | **0.84** ($\sigma \approx 0.05$) | Locked ðŸŸ¢ | | **Rot** (High Entropy) | **0.13** ($\sigma \approx 0.07$) | Slipped ðŸ”´ | Under the "Rot" condition, the Precursor Head decoupled from the local context, attending instead to static "sink tokens" (newlines and punctuation). The circuit physically failed to propagate the schema. <d-figure> <iframe src="/al-folio/assets/img/blog/l1h0_stability.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> <figcaption><strong>Figure 1:</strong> L1H0 Attention Stability. The precursor head's attention to the previous token (t-1) drops catastrophically under high-entropy conditions (Rot), from 0.84 to 0.13 mean attention weight.</figcaption> </d-figure> ### Causal Necessity Is L1H0 failure the *cause* or just a symptom? We performed a **Causal Patching** intervention. By surgically grafting the "healthy" L1H0 activation from the Control run into the Rot run, we recovered the correct output probability for the target token (`" France"`). This confirms necessity. <d-figure> <iframe src="/al-folio/assets/img/blog/causal_patching.html" frameborder="0" scrolling="no" height="500px" width="100%"></iframe> <figcaption><strong>Figure 2:</strong> Causal Patching Experiment. Restoring the healthy L1H0 activation recovers the model's ability to predict the correct target token, demonstrating the causal role of this attention head in maintaining schema coherence.</figcaption> </d-figure> --- ## 4. Experimental Trace: Macro-Behavioral Homology Does this matter for frontier models? We ran the same SAP protocol on **Claude 3.5 Sonnet** ($N=20$ trials, Temperature 0.0) with a strict JSON-only constraint. * **Control:** 100% Valid JSON. * **Rot:** **45% Failure Rate** (11/20 Success). Failures manifested as syntax errors (unclosed braces) and refusal loops. Despite the massive scale difference, the behavioral failure in Claude mirrors the mechanistic failure in GPT-2. The "Rot" consumes the attention budget required to maintain the JSON syntax contract. --- ## 5. The Tokenization Tax (Structural Bias) This mechanism highlights a systemic inequity. If "Entropy Load" ($E$) drives collapse, then languages with inefficient tokenization are inherently less safe. As noted by **Deng et al. (2024)** <d-cite key="deng2024multilingual"></d-cite>, low-resource languages often require 3-4x more tokens to express the same semantic concept. This structural fragmentation increases $E$ in the residual stream. $$E_{\text{Hindi}} &gt; E_{\text{English}} \implies \text{Safety Margin}_{\text{Hindi}} &lt; \text{Safety Margin}_{\text{English}}$$ Non-English speakers operate their agents closer to the Collapse Threshold ($\Theta$) by default. A prompt that is safe in English may trigger L1H0 saturation in Hindi simply due to the geometry of the tokenizer. --- ## 6. Conclusion We must move beyond "Vibes-based Evaluation." **Semiotic Collapse** is not a mystery; it is a circuit breaker tripping under load. For 2026, we advocate for **Entropy-Aware Safety**: 1. **Monitor Precursor Heads:** Real-time probing of L1H0 stability can predict hallucinations before they happen. 2. **Entropy Scrubbing:** Pre-processing layers must normalize the Key-Vector manifold to protect fragile induction circuits. Only by understanding the *physics* of the model can we build agents that uphold the Semiotic Contract in a high-entropy world. --- ### Acknowledgements This work utilized the **TransformerLens** library for mechanistic analysis. Code and data are available in the [accompanying repository](https://github.com/Tuesdaythe13th/semiotic_collapse). </d-article> </body></html>