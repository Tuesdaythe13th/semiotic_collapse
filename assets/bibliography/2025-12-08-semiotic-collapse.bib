@article{olsson2022context,
  title={In-context Learning and Induction Heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and Nova, Nova and Weeks, Kamal and Olah, Chris},
  journal={Transformer Circuits Thread},
  year={2022},
  url={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads}
}

@article{ahia2023cost,
  title={Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models},
  author={Ahia, Orevaoghene and Kumar, Sachin and Gholipour, Hila and et al.},
  journal={arXiv preprint arXiv:2305.13707},
  year={2023},
  url={https://arxiv.org/abs/2305.13707}
}

@techreport{rumbelow2023solidgold,
  title={SolidGoldMagikarp: Plus, why you should be careful with the token 160},
  author={Rumbelow, Jessica and Watkins, Matthew},
  institution={SERI MATS},
  year={2023},
  type={Technical Report},
  url={https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-why-you-should-be-careful-with-the}
}

@article{elhage2021framework,
  title={A Mathematical Framework for Transformer Circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and et al.},
  journal={Transformer Circuits Thread},
  year={2021},
  url={https://transformer-circuits.pub/2021/framework/index.html}
}

@inproceedings{petrov2023tokenizers,
  title={Language Model Tokenizers Introduce Unfairness Between Languages},
  author={Petrov, Aleksandar and La Malfa, Emanuele and Torr, Philip},
  booktitle={Proceedings of NeurIPS},
  year={2023},
  url={https://openreview.net/forum?id=DQ0an6FN4w}
}
